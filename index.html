<!DOCTYPE html>
<html lang="en">
<head>
  <title>Tom Monnier</title>
  <meta name="description" content="Personal webpage of Tom Monnier">
  <meta charset="utf-8">
  <link rel="icon" href="./resrc/icon.svg" type="image/svg">
  <meta http-equiv="Content-type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta name="format-detection" content="telephone=no">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">
  <link href="style.css" rel="stylesheet" type="text/css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
  <script async defer src="https://buttons.github.io/buttons.js"></script>

  <!-- Google Analytics (DO NOT copy/paste following section, setup your own analytics tag 
    at https://analytics.google.com/analytics/web/) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-RQGR4X733Q"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag("js", new Date());

  gtag("config", "G-RQGR4X733Q");
  </script>
  <!-- End of Google Analytics -->

</head>
<body>
  
<div class="container" style="text-align:center; padding-top:2rem;">
  <div class="row" style="text-align:left; margin-top:0.25em; display: flex; align-items: center; flex-wrap: wrap;">
    <div class="hidden-xs hidden-sm col-md-3">
      <img src="./resrc/tom_monnier_vert.jpg" alt="tom_monnier.jpg" style="width:85%; ">
    </div>
    <div class="col-xs-12 col-sm-12 col-md-9">
      <h2 style="margin-top:0em; margin-bottom:0.25em">Tom Monnier</h2>
      <h3 style="margin-top:0em; margin-bottom:0.5em;">Research Scientist at <a href="https://ai.meta.com">Meta</a>
      </h3>

      <p>
      I am a Research Scientist at <a href="https://ai.meta.com">Meta</a> working on computer vision, with an emphasis on 3D modeling and 3D generation.
      I did my PhD in the amazing <a href="https://imagine.enpc.fr">Imagine</a> lab 
      at <a href="https://www.ecoledesponts.fr/en">ENPC</a> under the guidance of Mathieu Aubry.
      During my PhD, I was fortunate to work with Jean Ponce (<a href="https://www.di.ens.fr/willow/">Inria</a>), Matthew Fisher 
      (<a href="https://research.adobe.com/">Adobe Research</a>), Alyosha Efros and Angjoo Kanazawa 
      (<a href="https://bair.berkeley.edu/">UC Berkeley</a>). Before that, I completed my engineer's degree (=M.Sc.) at
      <a href="http://www.mines-paristech.eu/">Mines Paris</a>.
      </p>

      <p>
      My research is focused on learning things from images without annotations, with a particular interest in recovering the underlying 3D
      <em class="highlight">(see representative papers)</em>. I am always looking for PhD interns, feel free to reach out!
      </p>

      <h4 style="margin-top:0.25em; margin-bottom:0em; padding-bottom:0em;">
      <a href="./email.html">email.</i></a>&emsp;
      <a href="https://github.com/monniert" target="_blank">github.</i></a>&emsp;
      <a href="https://scholar.google.com/citations?user=ZfV1DqMAAAAJ&hl=en" target="_blank">google scholar.</a>&emsp;
      <a href="https://twitter.com/t_monnier" target="_blank">twitter.</i></a>
      </h4>

    </div>
  </div>
  </div>
</div>

<!--
<div class="container">
  <h2>News</h2>
  <div class="limit-items">
    <input type="checkbox" id="show-all">
    <label for="show-all" class="text-show"><h6 style="margin:0em">Show more</h6></label>
    <label for="show-all" class="text-hide"><h6 style="margin:0em">Hide</h6></label>
    <div class="items">
      <ul>
        <li>
          <b class="item">10 / 2023</b>&emsp;
          Our multi-view approach to 3D decomposition called <a href="https://tmonnier.com/DBW/">Differentiable Blocks World</a>
          has been accepted at <a href="https://nips.cc/Conferences/2023">NeurIPS 2023</a>!
        </li>        
        <li>
          <b class="item">09 / 2023</b>&emsp;
          I am joining Meta in the Paris office as a research scientist
        </li>        
        <li>
          <b class="item">07 / 2023</b>&emsp;
          We release <a href="https://tmonnier.com/DBW/">Differentiable Blocks World</a> on arXiv, where 3D scenes are
          reconstructed by fitting textured primitives in multiple views
        </li>        
        <li>
          <b class="item">03 / 2023</b>&emsp;
          Our work <a href="https://imagine.enpc.fr/~guedona/MACARONS/">MACARONS</a> on simultaneous path planning and
          3D reconstruction has been accepted to <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>!
        </li>
        <li>
          <b class="item">02 / 2023</b>&emsp; <a href="https://arxiv.org/abs/2302.01660">The Learnable Typewriter</a> - a document-analysis
          system learned with no or weak supervision - is published on arXiv
        </li>
        <li>
          <b class="item">12 / 2022</b>&emsp;
          Our analytical paper on <a href="https://arxiv.org/abs/2212.10292">unsupervised visual reasoning</a> has been accepted at
          <a href="https://sslneurips22.github.io/">NeurIPS 2022 SSL Workshop</a>!
        </li>
        <li>
          <b class="item">07 / 2022</b>&emsp;
          The single-view reconstruction method <a href="./UNICORN/">UNICORN</a> &#129412;
          has been accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a>!
        </li>
        <li>
          <b class="item">06 / 2022</b>&emsp;
          Visiting <a href="https://www.berkeley.edu/">University of California</a> and presenting my PhD work at 
          <a href="https://bair.berkeley.edu/">Berkeley AI Research</a>
        </li>
        <li>
          <b class="item">10 / 2021</b>&emsp;
          Our work on <a href="https://romainloiseau.github.io/deep-linear-shapes/">representing 3D shapes</a>
          has been accepted at <a href="https://3dv2021.surrey.ac.uk/">3DV 2021</a>!
        </li>
        <li>
          <b class="item">07 / 2021</b>&emsp;
          Our <a href="./DTI-Sprites/">sprite-based image decomposition
            approach</a> has been accepted to <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a>!
        </li>
        <li>
          <b class="item">05 / 2021</b>&emsp;
          Starting a remote internship in the Creative Intelligence Lab at <a href="https://research.adobe.com/">Adobe
            Research</a> advised by <a href="https://techmatt.github.io">Matthew Fisher</a>
        </li>
        <li>
          <b class="item">12 / 2020</b>&emsp;
          Presenting <a href="./DTIClustering/">DTI Clustering</a> at a workshop for
          <a href="http://parismlgroup.org/">ParisMLGroup</a>
        </li>
        <li>
          <b class="item">11 / 2020</b>&emsp;
          Presenting <a href="./docExtractor/">docExtractor</a> at a 
          <a href="https://www.meshs.fr/page/dhnord2020">DHNord 2020</a> workshop
        </li>
        <li>
          <b class="item">10 / 2020</b>&emsp;
          Our work on <a href="./DTIClustering/">DTI Clustering</a> 
          is accepted at <a href="https://nips.cc/Conferences/2020">NeurIPS 2020</a> as an oral presentation!
        </li>
        <li>
          <b class="item">06 / 2020</b>&emsp;
          Our system <a href="./docExtractor/docExtractor.pdf">docExtractor</a> is accepted at <a href="http://icfhr2020.tu-dortmund.de/">ICFHR
          2020</a> as an oral!
        </li>
        <li>
          <b class="item">02 / 2020</b>&emsp;
          Starting PhD in the <a href="https://imagine.enpc.fr">Imagine</a> research group at 
          <a href="https://www.ecoledesponts.fr/en">ENPC</a> under the supervision of
          <a href="https://imagine.enpc.fr/~aubrym/">Mathieu Aubry</a>
        </li>
        <li>
          <b class="item">01 / 2020</b>&emsp;
          Our web application for watermark recognition is online, check out the
          <a href="https://hal.inria.fr/hal-02513038/document">paper</a> and test the app
          <a href="https://filigranes.inria.fr/#/filigrane-search">here</a>!
        </li>
      </ul>
    </div>
  </div>
</div>
-->

<div class="container">
  <h2>Publications</h2>

  <div class="row highlight" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <div class="hidden-xs col-xs-5 vcenter" style="text-align:center;">
      <a href="https://arxiv.org/abs/2503.08382" target="_blank">
        <img src="./resrc/25_wildcat3d.gif" alt="25_wildcat3d.gif" style="width:100%;max-width:300px">
      </a>
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild</b><br>
      <a href="https://morrisalp.github.io/">Morris Alper</a>,
      <a href="https://d-novotny.github.io/" rel="external nofollow noopener" target="_blank">David Novotny</a>,
      <a href="https://www.fkokkinos.com/" rel="external nofollow noopener" target="_blank">Filippos Kokkinos</a>,
      <a href="https://www.hadarelor.com/" rel="external nofollow noopener" target="_blank">Hadar Averbuch-Elor</a>,
      <u>Tom Monnier</u><br>
      <span>arxiv 2025</span><br>
      <a href="https://arxiv.org/abs/2506.13030">paper</a> | <a
         href="https://wildcat3d.github.io/">webpage</a>
      <p></p>
      <p>
      WildCAT3D learns from in-the-wild image collections with diverse appearances, enabling appearance-controlled novel-view synthesis from a single image.
      </p>
    </div>
  </div>

  <div class="row" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <div class="hidden-xs col-xs-5 vcenter" style="text-align:center;">
      <a href="https://arxiv.org/abs/2503.08382" target="_blank">
        <img src="./resrc/25_twinner.gif" alt="25_twinner.gif" style="width:100%;max-width:400px">
      </a>
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>Twinner: Shining Light on Digital Twins in a Few Snaps</b><br>
      <a href="https://www.linkedin.com/in/jesus-zarzar/">Jesus Zarzar</a>,
      <u>Tom Monnier</u>,
      <a href="https://www.shapovalov.ro/" rel="external nofollow noopener" target="_blank">Roman Shapovalov</a>,
      <a href="https://www.robots.ox.ac.uk/~vedaldi/" rel="external nofollow noopener" target="_blank">Andrea Vedaldi</a>,
      <a href="https://d-novotny.github.io/" rel="external nofollow noopener" target="_blank">David Novotny</a><br>
      <span>CVPR 2025</span><br>
      <a href="https://arxiv.org/pdf/2503.08382">paper</a>
      <p></p>
      <p>
      We introduce an LRM capable of recovering illumination, geometry and material properties of real object scenes from a few posed images.
      </p>
    </div>
  </div>

  <div class="row highlight" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <div class="hidden-xs col-xs-5 vcenter" style="text-align:center;">
      <a href="https://uco3d.github.io/" target="_blank">
        <video autoplay muted loop playsinline style="margin:auto;padding:0px;display:block;width:100%;">
          <source src="./resrc/25_uco3d_2.mp4" type="video/mp4">
        </video>
      </a>
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>UnCommon Objects in 3D</b><br>
      <a href="https://xingchenliu.com/">Xingchen Liu</a>,
      <a href="https://www.linkedin.com/in/piyush-tayal/?originalSubdomain=ca">Piyush Tayal</a>,
      <a href="https://jytime.github.io/">Jianyuan Wang</a>,
      <a href="https://www.linkedin.com/in/jesus-zarzar/">Jesus Zarzar</a>,
      <u>Tom Monnier</u>,
      <a href="https://ktertikas.github.io/">Konstantinos Tertikas</a>,
      <a href="https://davidsonic.github.io/index/">Jiali Duan</a>,
      <a href="https://www.antoinetlc.com/">Antoine Toisoul</a>,
      <a href="https://jasonyzhang.com/">Jason Y. Zhang</a>,
      <a href="https://ai.meta.com/people/847410227285977/natalia-neverova/" rel="external nofollow noopener" target="_blank">Natalia Neverova</a>, <a href="https://www.robots.ox.ac.uk/~vedaldi/" rel="external nofollow noopener" target="_blank">Andrea Vedaldi</a>, <a href="https://www.shapovalov.ro/" rel="external nofollow noopener" target="_blank">Roman Shapovalov</a>, <a href="https://d-novotny.github.io/" rel="external nofollow noopener" target="_blank">David Novotny</a><br>
      <span>CVPR 2025</span><br>
      <a href="https://arxiv.org/pdf/2501.07574">paper</a> | <a
         href="https://uco3d.github.io/">webpage</a> | <a
         href="https://github.com/facebookresearch/uco3d">code</a>
      <p></p>
      <p>
      We present a new object-centric dataset for 3D deep learning and 3D generative AI.
      </p>
    </div>
  </div>

  <div class="row" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <div class="hidden-xs col-sm-5 vcenter" style="text-align:center;">
      <a href="https://silent-chen.github.io/PartGen/" target="_blank">
          <video autoplay muted loop playsinline style="margin:auto;padding:0px;display:block;width:100%;max-width:300px;">
            <source src="./resrc/25_partgen_2.mp4" type="video/mp4">
          </video>
      </a>
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models</b><br>
      <a href="https://silent-chen.github.io/">Minghao Chen</a>,
      <a href="https://www.shapovalov.ro/" rel="external nofollow noopener" target="_blank">Roman Shapovalov</a>,
      <a href="https://scholar.google.de/citations?user=n9nXAPcAAAAJ&hl=en">Iro Laina</a>,
      <u>Tom Monnier</u>,
      <a href="https://jytime.github.io/">Jianyuan Wang</a>,
      <a href="https://d-novotny.github.io/" rel="external nofollow noopener" target="_blank">David Novotny</a>,
      <a href="https://www.robots.ox.ac.uk/~vedaldi/" rel="external nofollow noopener" target="_blank">Andrea Vedaldi</a><br>
      <span>CVPR 2025 <b>(Highlight award)</b></span><br>
      <a href="https://arxiv.org/abs/2412.18608">paper</a> | <a
         href="https://silent-chen.github.io/PartGen/">webpage</a>
      <p></p>
      <p>
      We propose a method for compositional part-level 3D generation and reconstruction from various modalities including text, image or 3D models.
      </p>
    </div>
  </div>

  <div class="row" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <div class="hidden-xs col-xs-5 vcenter" style="text-align:center;">
      <a href="https://ai.meta.com/research/publications/meta-3d-gen/" target="_blank">
        <img src="./resrc/24_3dgen.png" alt="24_3dgen.png" style="width:100%;max-width:300px">
      </a>
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>Meta 3D Gen</b><br>
      <a href="https://scholar.google.com/citations?user=ei3OmNkAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Raphael Bensadoun</a>, <u>Tom Monnier</u>, <a href="https://www.yanirk.com/" rel="external nofollow noopener" target="_blank">Yanir Kleiman</a>, <a href="https://fkokkinos.github.io/" rel="external nofollow noopener" target="_blank">Filippos Kokkinos</a>, <a href="https://nihalsid.github.io/">Yawar Siddiqui</a>, <a href="https://uk.linkedin.com/in/mahendrakariya" rel="external nofollow noopener" target="_blank">Mahendra Kariya</a>, <a href="https://il.linkedin.com/in/omriharosh?trk=public_profile_browsemap" rel="external nofollow noopener" target="_blank">Omri Harosh</a>, <a href="https://www.shapovalov.ro/" rel="external nofollow noopener" target="_blank">Roman Shapovalov</a>, <a href="https://fr.linkedin.com/in/emilien-garreau-b87606ab/en" rel="external nofollow noopener" target="_blank">Emilien Garreau</a>, <a href="https://akanimax.github.io/" rel="external nofollow noopener" target="_blank">Animesh Karnewar</a>, <a href="https://caoang327.github.io/" rel="external nofollow noopener" target="_blank">Ang Cao</a>, <a href="https://scholar.google.com/citations?user=HKfczk8AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Idan Azuri</a>, <a href="https://uk.linkedin.com/in/lvoursl" rel="external nofollow noopener" target="_blank">Iurii Makarov</a>, <a href="https://erictuanle.com/" rel="external nofollow noopener" target="_blank">Eric-Tuan Le</a>, <a href="https://www.antoinetlc.com/" rel="external nofollow noopener" target="_blank">Antoine Toisoul</a>, <a href="https://d-novotny.github.io/" rel="external nofollow noopener" target="_blank">David Novotny</a>, <a href="https://ai.meta.com/people/218463647991453/oran-gafni/" rel="external nofollow noopener" target="_blank">Oran Gafni</a>, <a href="https://ai.meta.com/people/847410227285977/natalia-neverova/" rel="external nofollow noopener" target="_blank">Natalia Neverova</a>, <a href="https://www.robots.ox.ac.uk/~vedaldi/" rel="external nofollow noopener" target="_blank">Andrea Vedaldi</a><br>
      <span>arXiv 2024</span><br>
      <a href="https://arxiv.org/pdf/2407.02599">paper</a> | <a
         href="https://ai.meta.com/research/publications/meta-3d-gen/">webpage</a> | <a
         href="https://www.facebook.com/AIatMeta/videos/447532454793563/">video</a>
      <p></p>
      <p>
      We combine Meta 3D AssetGen and TextureGen for high-quality mesh generation.
      </p>
    </div>
  </div>

  <div class="row highlight" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <div class="hidden-xs col-xs-5 vcenter" style="text-align:center;">
      <a href="https://assetgen.github.io/" target="_blank">
        <img src="./resrc/24_assetgen.gif" alt="24_assetgen.gif" style="width:100%;max-width:300px">
      </a>
    </div><!--


    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials</b><br>
      <a href="https://nihalsid.github.io/">Yawar Siddiqui</a>,
      <u>Tom Monnier</u>,
      <a href="https://fkokkinos.github.io/" rel="external nofollow noopener" target="_blank">Filippos Kokkinos</a>, <a href="https://uk.linkedin.com/in/mahendrakariya" rel="external nofollow noopener" target="_blank">Mahendra Kariya</a>, <a href="https://www.yanirk.com/" rel="external nofollow noopener" target="_blank">Yanir Kleiman</a>, <a href="https://fr.linkedin.com/in/emilien-garreau-b87606ab/en" rel="external nofollow noopener" target="_blank">Emilien Garreau</a>, <a href="https://ai.meta.com/people/218463647991453/oran-gafni/" rel="external nofollow noopener" target="_blank">Oran Gafni</a>, <a href="https://ai.meta.com/people/847410227285977/natalia-neverova/" rel="external nofollow noopener" target="_blank">Natalia Neverova</a>, <a href="https://www.robots.ox.ac.uk/~vedaldi/" rel="external nofollow noopener" target="_blank">Andrea Vedaldi</a>, <a href="https://www.shapovalov.ro/" rel="external nofollow noopener" target="_blank">Roman Shapovalov</a>, <a href="https://d-novotny.github.io/" rel="external nofollow noopener" target="_blank">David Novotny</a><br>
      <span>NeurIPS 2024</span><br>
      <a href="https://assetgen.github.io/static/AssetGen.pdf">paper</a> | <a
         href="https://assetgen.github.io/">webpage</a> | <a
         href="https://www.youtube.com/watch?v=xY_2jAEcBa0">video</a>
      <p></p>
      <p>
      We introduce a novel text- or image-conditioned generator of 3D assets with physically-based rendering materials and detailed geometry.
      </p>
    </div>
  </div>

  <div class="row" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <div class="hidden-xs col-xs-5 vcenter" style="text-align:center;">
      <a href="https://holodiffusion.github.io/goembed/" target="_blank">
        <img src="./resrc/24_goembed.jpg" alt="24_goembed.jpg" style="width:100%;max-width:300px">
      </a>
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>GOEmbed: Gradient Origin Embeddings for Representation Agnostic 3D Feature Learning</b><br>
      <a href="https://github.com/akanimax">Animesh Karnewar</a>,
      <a href="https://shapovalov.ro/">Roman Shapovalov</a>,
      <u>Tom Monnier</u>,
      <a href="https://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>,
      <a href="http://www0.cs.ucl.ac.uk/staff/n.mitra/">Niloy J. Mitra</a>,
      <a href="https://d-novotny.github.io/">David Novotny</a><br>
      <span>ECCV 2024</span><br>
      <a href="https://arxiv.org/abs/2312.08744">paper</a> | <a
         href="https://holodiffusion.github.io/goembed/">webpage</a>
      <p></p>
      <p>
      We propose a method that encodes 2D images into any 3D representation, without requiring pre-trained image feature extractor.
      </p>
    </div>
  </div>

  <div class="row highlight" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <div class="hidden-xs col-xs-5 vcenter" style="text-align:center;">
      <a href="https://hal.science/tel-04475696" target="_blank">
        <img src="./resrc/23_thesis.png" alt="23_thesis.png" style="width:100%;max-width:300px">
      </a>
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>Unsupervised Image Analysis by Synthesis</b><br>
      <u>Tom Monnier</u><br>
      <span>PhD thesis <b>(Accessit award from AFRIF)</b></span><br>
      <a href="./phd_thesis/monnier2023_phd_thesis.pdf">paper</a> | <a
         href="./bibtex/phd_thesis.bib">bibtex</a>
      <p></p>
      <p>
      This thesis studies unsupervised image analysis models, its central idea is to learn these models
      by synthesizing the images themselves.
      </p>
    </div>
  </div>


  <div class="row" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <div class="hidden-xs col-xs-5 vcenter" style="text-align:center;">
      <a href="http://imagine.enpc.fr/~siglidii/learnable-typewriter/" target="_blank">
        <img src="./resrc/learnable_typewriter.jpg" alt="learnable_typewriter.jpg" style="width:100%;max-width:300px">
      </a>
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>The Learnable Typewriter: A Generative Approach to Text Line Analysis</b><br>
      <a href="https://imagine.enpc.fr/~siglidii/">Ioannis Siglidis</a>,
      <a href="https://perso.telecom-paristech.fr/gonthier/">Nicolas Gonthier</a>,
      <a href="https://juliengaubil.github.io/">Julien Gaubil</a>,
      <u>Tom Monnier</u>,
      <a href="https://imagine.enpc.fr/~aubrym">Mathieu Aubry</a><br>
      <span>ICDAR 2024 <b>(Best paper award)</b></span><br>
      <a href="https://arxiv.org/abs/2302.01660">paper</a> | <a
         href="http://imagine.enpc.fr/~siglidii/learnable-typewriter/">webpage</a> | <a
         href="https://github.com/ysig/learnable-typewriter">code</a> | <a
         href="./bibtex/ltw.bib">bibtex</a>
      <p></p>
      <p>
      We build upon sprite-based image decomposition approaches to design a generative method for
      character analysis and recognition in text lines.
      </p>
    </div>
  </div>

  <div class="row highlight" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <div class="hidden-xs col-sm-5 vcenter" style="text-align:center;">
      <a href="./DBW/" target="_blank">
          <video autoplay muted loop playsinline style="margin:auto;padding:0px;display:block;width:100%;max-width:400px;">
            <source src="./resrc/dbw.mp4" type="video/mp4">
            <img src="./resrc/dbw.jpg" width=95% title="Your browser does not support videos.">
          </video>
      </a>
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>Differentiable Blocks World: Qualitative 3D Decomposition by Rendering Primitives</b><br>
      <u>Tom Monnier</u>,
      <a href="https://github.com/jake-austin">Jake Austin</a>,
      <a href="http://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>,
      <a href="http://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>,
      <a href="https://imagine.enpc.fr/~aubrym">Mathieu Aubry</a><br>
      <span>NeurIPS 2023</span><br>
      <a href="https://arxiv.org/abs/2307.05473">paper</a> | <a
         href="./DBW/">webpage</a> | <a
         href="https://github.com/monniert/differentiable-blocksworld">code</a> | <a
         href="./DBW/dbw.pptx">slides</a> | <a
         href="./DBW/ref.bib">bibtex</a>
      <p></p>
      <p>
      We compute a primitive-based 3D reconstruction from multiple views by optimizing 
      textured superquadric meshes with learnable transparency.
      </p>
    </div>
  </div>

  <div class="row" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <div class="hidden-xs col-xs-5 vcenter" style="text-align:center;">
      <a href="https://imagine.enpc.fr/~guedona/MACARONS/" target="_blank">
        <img src="./resrc/macarons.png" alt="macarons.png" style="width:100%;max-width:400px">
      </a>
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>MACARONS: Mapping And Coverage Anticipation with RGB Online Self-supervision </b><br>
      <a href="https://imagine.enpc.fr/~guedona/">Antoine Gu&eacute;don</a>,
      <u>Tom Monnier</u>,
      <a href="https://imagine.enpc.fr/~monasse/">Pascal Monasse</a>,
      <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a><br>
      <span>CVPR 2023</span><br>
      <a href="https://arxiv.org/abs/2303.03315">paper</a> | <a
         href="https://imagine.enpc.fr/~guedona/MACARONS/">webpage</a> | <a
         href="https://github.com/Anttwo/MACARONS">code</a> | <a
         href="https://youtu.be/NlUNFJYuBGs">video</a> | <a
         href="https://imagine.enpc.fr/~guedona/MACARONS/macarons.pptx">slides</a> | <a
         href="./bibtex/macarons.bib">bibtex</a>
      <p></p>
      <p>
      We introduce MACARONS, a method that learns in a self-supervised fashion to explore new environments and 
      reconstruct them in 3D using RGB images only.
      </p>
    </div>
  </div>

  <div class="row" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <div class="hidden-xs col-xs-5 vcenter" style="text-align:center;">
      <a href="https://arxiv.org/abs/2212.10292" target="_blank">
        <img src="./resrc/unsup_reasoning.png" alt="unsup_reasoning.png" style="width:100%;max-width:400px">
      </a>
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>Towards Unsupervised Visual Reasoning: Do Off-The-Shelf Features Know How to Reason?</b><br>
      <a href="https://wysoczanska.github.io/">Monika Wysoczanska</a>,
      <u>Tom Monnier</u>,
      <a href="http://staff.ii.pw.edu.pl/~ttrzcins/">Tomasz Trzcinski</a>,
      <a href="https://davidpicard.github.io/">David Picard</a><br>
      <span>NeurIPS Workshops 2022</span><br>
      <a href="https://arxiv.org/abs/2212.10292">paper</a> | <a
         href="./bibtex/unsup_reasoning.bib">bibtex</a>
      <p></p>
      <p>
      A Transformer-based framework to evaluate off-the-shelf features (object-centric and dense representations)
      for the reasoning task of VQA.
      </p>
    </div>
  </div>

  <div class="row highlight" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <div class="hidden-xs col-sm-5 vcenter" style="text-align:center;">
      <a href="./UNICORN/" target="_blank">
          <video autoplay muted loop playsinline style="margin:auto;padding:0px;display:block;width:100%;max-width:270px;">
            <source src="./resrc/unicorn.mp4" type="video/mp4">
            <img src="./resrc/unicorn.png" width=90% title="Your browser does not support videos.">
          </video>
      </a>
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency</b><br>
      <u>Tom Monnier</u>,
      <a href="https://techmatt.github.io">Matthew Fisher</a>,
      <a href="http://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>,
      <a href="https://imagine.enpc.fr/~aubrym">Mathieu Aubry</a><br>
      <span>ECCV 2022</span><br>
      <a href="https://arxiv.org/abs/2204.10310">paper</a> | <a
         href="./UNICORN/">webpage</a> | <a
         href="https://github.com/monniert/unicorn">code</a> | <a
         href="https://youtu.be/26N3CeVvZDs">video</a> | <a
         href="./UNICORN/unicorn.pptx">slides</a> | <a
         href="./UNICORN/ref.bib">bibtex</a>
      <p></p>
      <p>
      We present UNICORN, a self-supervised approach leveraging the consistency across
      different single-view images for high-quality 3D reconstructions.
      </p>
    </div>
  </div>

  <div class="row" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <!--onmouseout="dti3d_stop()" onmouseover="dti3d_show()">-->
    <div class="hidden-xs col-sm-5 vcenter" style="text-align:center;">
      <a href="https://romainloiseau.github.io/deep-linear-shapes/" target="_blank">
        <img src="./resrc/dti-3d.jpg" alt="dti-3d.jpg" style="width:100%;max-width:300px">
      </a>
      <!--
      <div class="one">
        <div class="two" id="dti3d_two" style="margin-top:0.3rem">
          <a href="https://romainloiseau.github.io/deep-linear-shapes/" target="_blank">
            <img src="./resrc/dti-3d_1.gif" alt="dti-3d_1.gif" style="width:20%;">
            <img src="./resrc/dti-3d_2.gif" alt="dti-3d_2.gif" style="width:20%;">
            <img src="./resrc/dti-3d_3.gif" alt="dti-3d_3.gif" style="width:20%;"><br>
            <img src="./resrc/dti-3d_4.gif" alt="dti-3d_4.gif" style="width:20%;">
            <img src="./resrc/dti-3d_5.gif" alt="dti-3d_5.gif" style="width:20%;">
            <img src="./resrc/dti-3d_6.gif" alt="dti-3d_6.gif" style="width:20%;">
          </a>
        </div>
        <a href="https://romainloiseau.github.io/deep-linear-shapes/" target="_blank">
          <img id="dti3d_one" src="./resrc/dti-3d.jpg" alt="dti-3d.jpg" style="width:81%">
        </a>
      </div>
      <script type="text/javascript">
        function dti3d_show() {
          document.getElementById("dti3d_one").style.opacity = "0";
          document.getElementById("dti3d_two").style.opacity = "1";
        }
        function dti3d_stop() {
          document.getElementById("dti3d_one").style.opacity = "1";
          document.getElementById("dti3d_two").style.opacity = "0";
        }
        dti3d_stop()
      </script>
      -->
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>Representing Shape Collections with Alignment-Aware Linear Models </b><br>
      <a href="https://romainloiseau.github.io/romainloiseau/">Romain Loiseau</a>,
      <u>Tom Monnier</u>,
      <a href="https://imagine.enpc.fr/~aubrym">Mathieu Aubry</a>,
      <a href="https://loiclandrieu.com/">Lo&iuml;c Landrieu</a><br>
      <span>3DV 2021</span><br>
      <a href="https://arxiv.org/abs/2109.01605">paper</a> | <a
         href="https://romainloiseau.github.io/deep-linear-shapes/">webpage</a> | <a
         href="https://github.com/romainloiseau/deep-linear-shapes">code</a> | <a
         href="./bibtex/dti-3d.bib">bibtex</a>
      <p></p>
      <p>
        We characterize 3D shapes as affine transformations of linear families learned without supervision, and showcase
        its advantages on large shape collections.
      </p>
    </div>
  </div>

  <div class="row" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <!--onmouseout="sprites_stop()" onmouseenter="sprites_start()" onmouseover="sprites_show()">-->
    <div class="hidden-xs col-sm-5 vcenter" style="text-align:center;">
      <a href="./DTI-Sprites/" target="_blank">
        <img src="./resrc/sprites.png" alt="sprites1.png" style="width:100%;max-width:350px">
      </a>
      <!--
      <div class="one">
        <div class="two" id="sprites_two" style="padding:0px;margin:0px">
          <a href="./DTI-Sprites/" target="_blank">
            <video id="sprites_video" width=88% autoplay muted loop playsinline
              style="margin:auto;padding-top:0rem;display:block;padding-left:5rem">
              <source src="./resrc/sprites.mp4" type="video/mp4">
              <img src="./resrc/sprites2.png" width=90% title="Your browser does not support videos.">
            </video>
          </a>
        </div>
        <a href="./DTI-Sprites/" target="_blank">
          <img id="sprites_one" src="./resrc/sprites.png" alt="sprites1.png" style="width:90%;">
        </a>
      </div>
      <script type="text/javascript">
        function sprites_start() {
          document.getElementById("sprites_one").style.opacity = "0";
          document.getElementById("sprites_two").style.opacity = "1";
          document.getElementById("sprites_video").currentTime = 0;
          document.getElementById("sprites_video").play();
        }
        function sprites_show() {
          document.getElementById("sprites_one").style.opacity = "0";
          document.getElementById("sprites_two").style.opacity = "1";
        }
        function sprites_stop() {
          document.getElementById("sprites_one").style.opacity = "1";
          document.getElementById("sprites_two").style.opacity = "0";
        }
        sprites_stop()
      </script>
      -->
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>Unsupervised Layered Image Decomposition into Object Prototypes</b><br>
      <u>Tom Monnier</u>, <a href="https://imagine.enpc.fr/~vincente">Elliot Vincent</a>,
      <a href="https://www.di.ens.fr/~ponce/">Jean Ponce</a>,
      <a href="https://imagine.enpc.fr/~aubrym">Mathieu Aubry</a><br>
      <span>ICCV 2021</span><br>
      <a href="https://arxiv.org/abs/2104.14575">paper</a> | <a
         href="./DTI-Sprites/">webpage</a> | <a
         href="https://github.com/monniert/dti-sprites">code</a> | <a
         href="https://www.youtube.com/watch?v=ZalsST1m5tU">video</a> | <a
         href="./DTI-Sprites/dtisprites.pptx">slides</a> | <a
         href="./DTI-Sprites/ref.bib">bibtex</a>
      <p></p>
      <p>
        We discover the objects recurrent in unlabeled image collections 
        by modeling images as a composition of learnable sprites.
      </p>
    </div>
  </div>

  <div class="row highlight" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <!--onmouseout="dtic_stop()" onmouseenter="dtic_start()" onmouseover="dtic_show()">-->
    <div class="hidden-xs col-sm-5 vcenter" style="text-align:center;">
      <a href="./DTIClustering/" target="_blank">
        <img src="./resrc/dtic2.png" alt="dtic.png" style="width:100%;max-width:400px">
      </a>
      <!--
      <div class="one">
        <div class="two" id="dtic_two">
          <a href="./DTIClustering/" target="_blank">
            <video id="dtic_video" width=100% autoplay muted loop playsinline
              style="margin:auto;padding-top:2rem;display:block;"><source src="./resrc/dtic.mp4" type="video/mp4">
              <img src="./resrc/dtic.png" width=90% title="Your browser does not support videos.">
            </video>
          </a>
        </div>
        <a href="./DTIClustering/" target="_blank">
          <img id="dtic_one" src="./resrc/dtic2.png" alt="dtic.png" style="width:100%;">
        </a>
      </div>
      <script type="text/javascript">
        function dtic_start() {
          document.getElementById("dtic_one").style.opacity = "0";
          document.getElementById("dtic_two").style.opacity = "1";
          document.getElementById("dtic_video").currentTime = 0;
          document.getElementById("dtic_video").play();
        }
        function dtic_show() {
          document.getElementById("dtic_one").style.opacity = "0";
          document.getElementById("dtic_two").style.opacity = "1";
        }
        function dtic_stop() {
          document.getElementById("dtic_one").style.opacity = "1";
          document.getElementById("dtic_two").style.opacity = "0";
        }
        dtic_stop()
      </script>
      -->
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>Deep Transformation-Invariant Clustering</b><br>
      <u>Tom Monnier</u>, <a href="https://imagine.enpc.fr/~groueixt">Thibault Groueix</a>,
      <a href="https://imagine.enpc.fr/~aubrym">Mathieu Aubry</a><br>
      <span>NeurIPS 2020 <b>(Oral award)</b></span><br>
      <a href="https://arxiv.org/abs/2006.11132">paper</a> | <a
         href="./DTIClustering/">webpage</a> | <a
         href="https://github.com/monniert/dti-clustering">code</a> | <a
         href="https://www.youtube.com/watch?v=j20MBc1hWGQ">video</a> | <a
         href="./DTIClustering/dtic_long.pptx">slides</a> | <a
         href="./DTIClustering/ref.bib">bibtex</a>
      <p></p>
      <p>
        A simple adaptation of K-means to make it work on pixels! We align prototypes to 
        each sample image before computing cluster distances.
      </p>
    </div>
  </div>

  <div class="row" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <div class="hidden-xs col-sm-5 vcenter" style="text-align:center;">
      <a href="./docExtractor/" target="_blank">
        <img src="./resrc/docExtractor.jpg" alt="docExtractor.jpg" style="width:100%;max-width:400px;">
      </a>
    </div><!--

    --><div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>docExtractor: An off-the-shelf historical document element extraction</b><br>
      <u>Tom Monnier</u>,
      <a href="https://imagine.enpc.fr/~aubrym">Mathieu Aubry</a><br>
      <span>ICFHR 2020 <b>(Oral award)</b></span><br>
      <a href="https://arxiv.org/abs/2012.08191">paper</a> |
      <a href="./docExtractor/">webpage</a> |
      <a href="https://github.com/monniert/docExtractor">code</a> |
      <a href="https://www.youtube.com/watch?v=Tuw8uQonW7E">video</a> |
      <a href="./docExtractor/slides_icfhr2020.pptx">slides</a> |
      <a href="./docExtractor/ref.bib">bibtex</a>
      <p></p>
      <p>
        Leveraging synthetic training data to efficiently extract visual elements from
        historical document images.
      </p>
    </div>
  </div>

  <!--
  <div class="row" style="text-align:left; margin-bottom:0.6em; margin-top:0.6em;">
    <div class="hidden-xs col-xs-5 vcenter" style="text-align:center;">
      <a href="https://hal.inria.fr/hal-02513038/document" target="_blank">
        <img src="./resrc/watermark.jpg" alt="watermark.jpg" style="width:100%;max-width:250px">
      </a>
    </div>

    <div class="col-xs-12 col-sm-7 vcenter" style="margin-top:0.5em">
      <b>A web application for watermark recognition</b><br>
      <a href="https://fr.linkedin.com/in/oumayma-bounou-23bb28113">O. Bounou</a>,
      <u>T. Monnier</u>,
      <a href="https://www.chartes.psl.eu/en/ilaria-pastrolin">I. Pastrolin</a>,
      <a href="https://imagine.enpc.fr/~shenx/">X. Shen</a>,
      <a href="http://www.chartes.psl.eu/fr/christine-benevent">C. B&eacute;n&eacute;vent</a> and others<br>
      <span>JDMDH 2020</span><br>
      <a href="https://hal.inria.fr/hal-02513038/document">paper</a> | <a
         href="https://filigranes.inria.fr/#/filigrane-search">web application</a> | <a
         href="https://arxiv.org/abs/1908.10254">related paper (Shen et al.)</a>
       <p></p>
       <p>New public web application dedicated to automatic watermark recognition.</p>
    </div>
  </div>
  -->
</div>

<div class="container">
  <h2>Academic activities</h2>
  <ul style="padding-left:0em; list-style-type:none;">
    <li><b>Visiting student</b>, <a href="https://www.berkeley.edu/">University of California</a>, Berkeley (USA) July 2022</li>
    <li><b>Research intern</b>, <a href="https://research.adobe.com/">Adobe Research</a>, San Francisco (USA, remote), Summer 2021</li>
    <li><b>Teaching assistant</b>, DEEPL (Masters level) at <a href="https://www.ecoledesponts.fr/en">ENPC</a>, Fall 2020 and Spring 2021</li>
    <li><b>Reviewer @</b> CVPR, ICCV, ECCV, NeurIPS, MVA</li>

    <!-- <li><b>Open source projects:</b></li> -->
    <!-- <a class="github-button" -->
    <!--    href="https://github.com/monniert/differentiable-blocksworld" -->
    <!--    data-size="large" -->
    <!--    data-show-count="true" -->
    <!--    data-count-aria-label="# stargazers on GitHub" -->
    <!--    aria-label="Star monniert/differentiable-blocksworld on GitHub">differentiable-blocksworld</a> -->
    <!-- <a class="github-button" -->
    <!--    href="https://github.com/monniert/docExtractor" -->
    <!--    data-size="large" -->
    <!--    data-show-count="true" -->
    <!--    data-count-aria-label="# stargazers on GitHub" -->
    <!--    aria-label="Star monniert/docExtractor on GitHub">docExtractor</a> -->
    <!-- <a class="github-button" -->
    <!--    href="https://github.com/monniert/dti-clustering" -->
    <!--    data-size="large" -->
    <!--    data-show-count="true" -->
    <!--    data-count-aria-label="# stargazers on GitHub" -->
    <!--    aria-label="Star monniert/dti-clustering on GitHub">dti-clustering</a> -->
    <!-- <a class="github-button" -->
    <!--    href="https://github.com/monniert/dti-sprites" -->
    <!--    data-size="large" -->
    <!--    data-show-count="true" -->
    <!--    data-count-aria-label="# stargazers on GitHub" -->
    <!--    aria-label="Star monniert/dti-sprites on GitHub">dti-sprites</a> -->
    <!-- <a class="github-button" -->
    <!--    href="https://github.com/monniert/project-webpage" -->
    <!--    data-size="large" -->
    <!--    data-show-count="true" -->
    <!--    data-count-aria-label="# stargazers on GitHub" -->
    <!--    aria-label="Star monniert/project-webpage on GitHub">project-webpage</a> -->
    <!-- <a class="github-button" -->
    <!--    href="https://github.com/monniert/unicorn" -->
    <!--    data-size="large" -->
    <!--    data-show-count="true" -->
    <!--    data-count-aria-label="# stargazers on GitHub" -->
    <!--    aria-label="Star monniert/unicorn on GitHub">unicorn</a> --> 
  </ul>

  <!--
  <h2>Invited talks</h2>
  <ul style="padding-left:0em; list-style-type:none;">
    <li><b class="item">06 / 2023</b>&emsp; <b>Job talk</b>, <a href="https://ai.facebook.com">Meta</a> (virtual), about the PhD thesis
    </li>
    <li><b class="item">06 / 2023</b>&emsp; <b>Job talk</b>, <a href="https://research.google/teams/perception/">Google Research</a> (virtual), about the PhD thesis
    </li>
    <li><b class="item">07 / 2022</b>&emsp; <b>Seminar</b>, <a href="https://bair.berkeley.edu/">BAIR - University of California</a> (Berkeley, USA), about the PhD thesis
    </li>
    <li><b class="item">04 / 2022</b>&emsp; <b>Seminar</b>, <a href="https://imagine-lab.enpc.fr/">Imagine</a> (La Turballe, France), about <em>"Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency"</em>
    </li>
    <li><b class="item">03 / 2021</b>&emsp; <b>Seminar</b>, <a href="https://dhai-seminar.github.io/">DHAI - Ecole Normale Sup&eacute;rieure</a> (virtual), about <em>"docExtractor: An off-the-shelf historical document element extraction"</em>
    </li>
    <li><b class="item">02 / 2021</b>&emsp; <b>Seminar</b>, <a href="http://ligm.u-pem.fr/accueil/">LIGM - ESIEE</a> (Paris, France), about <em>"Deep Transformation-Invariant Clustering"</em>
    </li>
    <li><b class="item">12 / 2020</b>&emsp; <b>Workshop</b>, <a href="http://parismlgroup.org/">Paris Machine Learning Group</a> (virtual), about <em>"Deep Transformation-Invariant Clustering"</em>
    </li>
    <li><b class="item">12 / 2020</b>&emsp; <b>Conference oral</b>, <a href="https://nips.cc/Conferences/2020">NeurIPS 2020</a> (virtual), about <em>"Deep Transformation-Invariant Clustering"</em>
    </li>
    <li><b class="item">11 / 2020</b>&emsp; <b>Workshop</b>, <a href="https://www.meshs.fr/page/dhnord2020">DHNord 2020</a> (virtual), about <em>"docExtractor: An off-the-shelf historical document element extraction"</em>
    </li>
    <li><b class="item">09 / 2020</b>&emsp; <b>Conference oral</b>, <a href="http://icfhr2020.tu-dortmund.de/">ICFHR 2020</a> (virtual), about <em>"docExtractor: An off-the-shelf historical document element extraction"</em>
    </li>
  </ul>
</div>
--!>

<div class="container" style="padding-top:1em; padding-bottom:2rem">
  <small>
    <p style="text-align:center">
      <!--
      Template inspired from
      <a href="https://imagine.enpc.fr/~groueixt" target="_blank">[1]</a>,
      <a href="https://www.cs.cmu.edu/~junyanz/" target="_blank">[2]</a>,
      <a href="https://jonbarron.info/" target="_blank">[3]</a>,
      <a href="https://people.eecs.berkeley.edu/~kanazawa/" target="_blank">[4]</a>.
      Misspellings: monier, monnie, monie, monniert. -->
      Last updated: July 2025
    </p>
  </small>
</div>

</body>
</html>
